{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-hJJC6BtL5tv"
      },
      "outputs": [],
      "source": [
        "!pip install llama-index pypdf sentence_transformers -q docx2txt"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using OpenAI GPT-3 model"
      ],
      "metadata": {
        "id": "6SWZroWDMhgM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import openai\n",
        "\n",
        "openai.api_key = \"\"\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"\"\n"
      ],
      "metadata": {
        "id": "gN0QNJhjMVwy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Indexing"
      ],
      "metadata": {
        "id": "91t2f5DhMnL9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index import VectorStoreIndex, SimpleDirectoryReader\n",
        "# load document from a directory\n",
        "documents = SimpleDirectoryReader('test').load_data()\n",
        "\n",
        "# create an index from the documents\n",
        "index = VectorStoreIndex.from_documents(documents)\n",
        "\n",
        "# create a query engine from the index\n",
        "query_engine = index.as_query_engine()\n",
        "\n",
        "# Query the engine\n",
        "response = query_engine.query(\"What is the letter about?\")\n",
        "print(response)"
      ],
      "metadata": {
        "id": "1NK2FK_zMeCA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Saving and loading index"
      ],
      "metadata": {
        "id": "c8fs97Quhibi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "index.storage_context.persist(\"naval_index\")"
      ],
      "metadata": {
        "id": "lhGKDnJ9hiFb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index import StorageContext, load_index_from_storage\n",
        "\n",
        "# rebuild storage context\n",
        "storage_context = StorageContext.from_defaults(persist_dir=\"naval_index\")\n",
        "# load index\n",
        "new_index = load_index_from_storage(storage_context)\n",
        "\n",
        "new_query_engine = new_index.as_query_engine()\n",
        "response = new_query_engine.query(\"who is this text about?\")\n",
        "print(response)\n"
      ],
      "metadata": {
        "id": "zpmbqXFGmiog"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Customizing LLMs using LangChain"
      ],
      "metadata": {
        "id": "IzCoGcIepLNt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index import LLMPredictor, ServiceContext\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "\n",
        "# Create a predictor using a custom model\n",
        "llm_predictor = LLMPredictor(llm=ChatOpenAI(temperature=0, model_name=\"gpt-3.5-turbo\"))\n",
        "\n",
        "# Create a service context with the custom predictor\n",
        "service_context = ServiceContext.from_defaults(llm_predictor=llm_predictor)\n",
        "\n",
        "# Create an index using the service context\n",
        "custom_llm_index = VectorStoreIndex.from_documents(\n",
        "    documents, service_context=service_context\n",
        ")\n",
        "\n",
        "custom_llm_query_engine = custom_llm_index.as_query_engine()\n",
        "response = custom_llm_query_engine.query(\"who is this text about?\")\n",
        "print(response)"
      ],
      "metadata": {
        "id": "PROaknOxpKYK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Custom Prompt\n",
        "for more structured responses"
      ],
      "metadata": {
        "id": "-8owuVOsqA5O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index import Prompt\n",
        "\n",
        "# Define a custom prompt\n",
        "template = (\n",
        "    \"We have provided context information below. \\n\"\n",
        "    \"---------------------\\n\"\n",
        "    \"{context_str}\"\n",
        "    \"\\n---------------------\\n\"\n",
        "    \"Given this information, please answer the question and each answer should start with code word AI Demos: {query_str}\\n\"\n",
        ")\n",
        "qa_template = Prompt(template)\n",
        "\n",
        "# Use the custom prompt when querying\n",
        "query_engine = custom_llm_index.as_query_engine(text_qa_template=qa_template)\n",
        "response = query_engine.query(\"who is this text about?\")\n",
        "print(response)\n"
      ],
      "metadata": {
        "id": "LCieV5zTrGGI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Custom embedding"
      ],
      "metadata": {
        "id": "UcUDsyCfr0yY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.embeddings.huggingface import HuggingFaceEmbeddings\n",
        "from llama_index import LangchainEmbedding, ServiceContext\n",
        "\n",
        "# Load in a specific embedding model\n",
        "embed_model = LangchainEmbedding(HuggingFaceEmbeddings(model_name='sentence-transformers/all-MiniLM-L6-v2'))\n",
        "\n",
        "# Create a service context with the custom embedding model\n",
        "service_context = ServiceContext.from_defaults(embed_model=embed_model)\n",
        "\n",
        "# Create an index using the service context\n",
        "new_index = VectorStoreIndex.from_documents(\n",
        "    documents,\n",
        "    service_context=service_context,\n",
        ")\n",
        "\n",
        "query_engine = new_index.as_query_engine()\n",
        "response = query_engine.query(\"list 5 important points from this letter\")\n",
        "print(response)"
      ],
      "metadata": {
        "id": "Hg4p3mGFr0gu"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}